<!DOCTYPE html>
<html lang="zh-CN">

  <head>
  <meta charset="utf-8">
  <meta name="author" content="zchengsite, 1451426471@qq.com" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  
  <title>Deterministic Policy Gradient (DPG) | To be a legend</title>

  
    <link rel="apple-touch-icon" href="/images/favicon.png">
    <link rel="icon" href="/images/favicon.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/iconfont/iconfont.css">
<link rel="stylesheet" href="/css/github-markdown.css">
<link rel="stylesheet" href="/css/highlight.css">


  <!-- jquery3.3.1 -->
  <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

  <!-- fancybox -->
  <link href="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.css" rel="stylesheet">
  <script async src="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>


  

<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="To be a legend" type="application/atom+xml">
</head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/"><img src="/images/avatar.png" alt=""></a>
    <div class="nickname"><a href="/">To be a legend</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">主页</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">归档</a>
        </li>
      
        <li class="nav-item" data-path="/categories/">
          <a href="/categories/">分类</a>
        </li>
      
        <li class="nav-item" data-path="/tags/">
          <a href="/tags/">标签</a>
        </li>
      
        <li class="nav-item" data-path="/tools/">
          <a href="/tools/">实用工具</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">关于</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->



  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">Deterministic Policy Gradient (DPG)</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime" title="更新时间 | updatetime"></i>
          2021-02-15
        </span>
        
              <span class="post-tags">
                <i class="iconfont icon-tags" title="标签 | tags"></i>
                
                <span class="span--tag">
                  <a href="/tags/Reinforcement-Learning/" title="查看 Reinforcement Learning 标签">
                    <b>#</b> Reinforcement Learning
                  </a>
                </span>
                
              </span>
          
      </div>
      <div class="markdown-body">
        <p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v32/silver14.pdf">Paper Link</a></p>
<p>Deterministic Policy Gradient (DPG) is a special policy gradient method which can solve the tasks with deterministic policies.  DPG has an appealing form : it is the expected gradient of the action-value function. This simple form means that the deterministic gradient can be estimated much more efficiently than the usual stochastic policy gradient.</p>
<h2 id="1-Stochastic-Actor-Critic-Algorithms"><a href="#1-Stochastic-Actor-Critic-Algorithms" class="headerlink" title="1 Stochastic Actor-Critic Algorithms"></a>1 Stochastic Actor-Critic Algorithms</h2><p>We first review traditional stochastic actor-critic algorithms, including on-policy actor-critic and off-policy actor-critic.</p>
<h3 id="On-Policy-Actor-Critic"><a href="#On-Policy-Actor-Critic" class="headerlink" title="On-Policy Actor-Critic"></a>On-Policy Actor-Critic</h3><p>According to the policy gradient theorem, we can compute the gradient of the current policy $\pi_\theta$ as follows:</p>
<script type="math/tex; mode=display">
\bigtriangledown_{\theta}J(\pi_{\theta}) = E_{s \sim \rho^{\pi_\theta} , a \sim \pi_\theta} \left[\bigtriangledown_{\theta}log\pi_{\theta}(a|s)Q^{\omega}(s, a)\right].\tag{1}</script><p>This equation consists of two approximators:  policy approximator $\pi_{\theta}$ and action-value approximator $Q^{\omega}$. The policy approximator is referred to as actor and the update of its parameters follow $\bigtriangledown_{\theta}J(\pi_{\theta})$, while the action-value approximator is  referred to as critic and the update of its parameters follow $\bigtriangledown_{\omega} E_{s \sim \rho^{\pi_\theta} , a \sim \pi_\theta} \left[(Q^{\omega}(s,a) - Q^{\pi}(s,a)^2)\right]$, where $Q^{\pi}$ is the true action-value. In contrast to using $Q^{\pi}$, using $Q^{\omega}$ may introduce bias but can reduce deviation effectively.</p>
<h3 id="Off-Policy-Actor-Critic"><a href="#Off-Policy-Actor-Critic" class="headerlink" title="Off-Policy Actor-Critic"></a>Off-Policy Actor-Critic</h3><p>In an off-policy setting, we can easily train the critic by using the temporal-difference error, however some modification is required for the estimation of the policy gradient.</p>
<p>The performance objective is modified to be the value function of the target policy $\pi$, averaged over the state distribution of the behavior policy $\beta$:</p>
<script type="math/tex; mode=display">\begin{align*}J_{\beta}(\pi_{\theta}) & = \int_{S} \rho^{\beta}(s)V^{\pi}(s)ds\\ & = \int_{S}\int_{A} \rho^{\beta}(s)\pi_{\theta}(a|s)Q^{\pi}(s,a)dads.\end{align*}\tag{2}</script><p>Differentiating above objective and applying an approximation gives the off-policy-gradient:</p>
<script type="math/tex; mode=display">
\begin{align*}
\bigtriangledown_{\theta}J_{\beta}(\pi_{\theta})
& \approx
\int_{S}\int_{A} \rho^{\beta}(s)\bigtriangledown_{\theta}
\pi_{\theta}(a|s)Q^{\pi}(s,a)dads
\\
& =
E_{s \sim \rho^{\beta}, a \sim \beta}
\left[
\frac{\pi_{\theta}(a|s)}{\beta_{\theta}(a|s)}
\bigtriangledown_{\theta}
log\pi_{\theta}(a|s)
Q^{\pi}(s,a)
\right]
\end{align*}
\tag{3}</script><p>Though the approximation drops a term that depends on the action-value gradient $\bigtriangledown_{\theta}Q^{\pi}(s,a)$, it is a good approximation since it can preserve the set of local optima to which gradient ascent converges. Note that the true action-value is unknown so we must use an approximator $V^v$ to approximate $V^{\pi}$and use the temporal-difference $r(s,a) + \gamma V^{v}(s’)$ to approximate $Q^{\pi}(s,a)$.</p>
<h2 id="2-Deterministic-Actor-Critic-Algorithms"><a href="#2-Deterministic-Actor-Critic-Algorithms" class="headerlink" title="2 Deterministic Actor-Critic Algorithms"></a>2 Deterministic Actor-Critic Algorithms</h2><p>Analogous to the stochastic actor-critic algorithms, we derive on-policy deterministic ac and off-policy deterministic ac respectively. Before the derivation, we first introduce the  deterministic policy gradient.</p>
<h3 id="Deterministic-Policy-Gradient"><a href="#Deterministic-Policy-Gradient" class="headerlink" title="Deterministic Policy Gradient"></a>Deterministic Policy Gradient</h3><p>We consider a deterministic policy $\mu_{\theta}: S \rightarrow A$ with parameter $\theta \in R^n$. We define a performance objective $J(\mu_{\theta}) = E\left[r^{\gamma}|\mu\right]$, and define probability distribution $p(s \rightarrow s’,t,\mu)$ and discounted state distribution $\rho^{\mu}(s)$ analogously to the stochastic case. Then let us write the performance objective as an expectation:</p>
<script type="math/tex; mode=display">
\begin{align*}
J(\mu_{\theta})
&=
\int_{S} \rho^{\mu}(s)r(s, \mu_{\theta}(s))ds
\\
&= 
E_{s \sim \rho^{\mu}}
[r(s,\mu_{\theta}(s))]
\end{align*}
\tag{4}</script><p>If $\triangledown_{\theta}\mu_{\theta}(s)$ and $\triangledown_{a}Q^{\mu}(s,a)$ exist, the deterministic policy gradient exists:</p>
<script type="math/tex; mode=display">
\begin{align*}
\triangledown_{\theta}J(\mu_{\theta})
&=
\int_{S}\rho^{\mu}(s)
\triangledown_{\theta}\mu_{\theta}(s)
\triangledown_{a}Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)}ds
\\
&=
E_{s \sim \rho^{\mu}}
\left[
\triangledown_{\theta}\mu_{\theta}(s)
\triangledown_{a}Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)}
\right]
\end{align*}
\tag{5}</script><p>Interestingly, we can also prove that the deterministic policy gradient is limiting case of the stochastic policy gradient. Specifically, we parameterize stochastic policies $\pi_{\mu_{\theta},\sigma}$ by a deterministic policy $\mu_{\theta}: S \rightarrow A$ and a variance parameter $\sigma$, such that for $\sigma=0$ the stochastic policy is equivalent to the deterministic policy, then as $\sigma \rightarrow 0$ the stochastic policy gradient converges to the deterministic gradient.</p>
<h3 id="On-Policy-Deterministic-Actor-Critic"><a href="#On-Policy-Deterministic-Actor-Critic" class="headerlink" title="On-Policy Deterministic Actor-Critic"></a>On-Policy Deterministic Actor-Critic</h3><p>Like the stochastic actor-critic, the deterministic actor-critic consists of two components. The critic estimates the action-value function while the actor ascends the gradient of the action-value function. Specifically, an actor adjusts the parameter $\theta$ of the deterministic policy $\mu_{\theta}$ by stochastic gradient ascent of Equation 5. As in the stochastic actor-critic, we use a differentiable action-value function $Q^{\omega}(s,a)$ in place of the true action-value function $Q^{\mu}(s,a)$. The critic uses Sarsa updates to estimate the action-value function. The parameters of actor ($\theta$) and the parameters of critic ($\omega$) are updated as follows:</p>
<script type="math/tex; mode=display">
\begin{align*}
\delta_{t}
&=
r_t + \gamma Q^{\omega}(s_{t+1},a_{t+1}) - Q^{\omega}(s_t,a_t)
\tag{6}
\\
\omega_{t+1}
&=
\omega_t + \alpha_{\omega}\delta_{t}\triangledown_{\omega}Q^{\omega}(s_t,a_t)
\tag{7}
\\
\theta_{t+1}
&=
\theta_{t} + \alpha_{\theta}
\triangledown_{\theta}\mu_{\theta}(s)
\triangledown_{a}Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)}
\tag{8}
\end{align*}</script><h3 id="Off-Policy-Deterministic-Actor-Critic"><a href="#Off-Policy-Deterministic-Actor-Critic" class="headerlink" title="Off-Policy Deterministic Actor-Critic"></a>Off-Policy Deterministic Actor-Critic</h3><p>To derive the off-policy deterministic actor-critic, we need to modify the performance objective to be the value function of the target policy, averaged over the state distribution of the behavior policy:</p>
<script type="math/tex; mode=display">
\begin{align*}
J_{\beta}(\mu_{\theta})
&=
\int_{S}
\rho^{\beta}(s)V^{\mu}(s)ds
\\
&=
\int_{S}
\rho^{\beta}(s)
Q^{\mu}(s,\mu_{\theta}(s))ds
\end{align*}
\tag{9}</script><p>The gradient of that objective is:</p>
<script type="math/tex; mode=display">
\begin{align*}
\triangledown_{\theta}J_{\beta}(\mu_\theta)
& \approx
\int_{S}\rho^{\beta}(s)
\triangledown_{\theta}\mu_{\theta}(a|s)Q^{\mu}(s,a)ds
\\
&=
E_{s \sim \rho^{\beta}}
\left[
\triangledown_{\theta}\mu_{\theta}(s)
\triangledown_{a}Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)}
\right]
\end{align*}
\tag{10}</script><p>This equation gives the off-policy deterministic policy gradient. Analogous to the stochastic case (Equation 3), we have dropped a term that depends on $\triangledown_{\theta}Q^{\mu_{\theta}}(s,a)$.</p>
<h2 id="DPG-Algorithm"><a href="#DPG-Algorithm" class="headerlink" title="DPG Algorithm"></a>DPG Algorithm</h2><div style="width:70%;margin:auto">
  <img src="/2021/02/15/Deterministic-Policy-Gradient-DPG/1.png" class="" title="Figure 1">
</div>

<p>Figure 1 outlines the DPG algorithm clearly. As shown in Figure 1, DPG contains two function approximator: actor $\mu_{\theta}$ and critic $Q_{\psi}$. Actor is updated followed by the Equation 10 where $Q_{\mu}$ is substituted with $Q_{\psi}$ , and critic is updated followed by the gradient of MSE.</p>
<p>Please note that DPG uses linear function approximators to approximate $Q_{\psi}$ and uses <em>COPDAC-Q</em> algorithm to update it, which can ensure the convergency of the DPG algorithm. We don’t elaborate it here because DDPG extends DPG to work with non-linear function approximators.</p>

      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/2021/02/01/Deep-Q-Network-DQN/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>PREV</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime" title="更新时间 | updatetime"></i>
              2021-02-15
            </span>
            
                  <span class="post-tags">
                    <i class="iconfont icon-tags" title="标签 | tags"></i>
                    
                    <span class="span--tag">
                      <a href="/tags/Reinforcement-Learning/" title="查看 Reinforcement Learning 标签">
                        <b>#</b> Reinforcement Learning
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/2021/02/28/NumPy%E7%AE%80%E6%98%93%E6%95%99%E7%A8%8B/" target="_self">
                <span>NEXT</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div class="post-catalog" id="catalog">
    <div class="title">目录</div>
    <div class="catalog-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Stochastic-Actor-Critic-Algorithms"><span class="toc-text">1 Stochastic Actor-Critic Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#On-Policy-Actor-Critic"><span class="toc-text">On-Policy Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Off-Policy-Actor-Critic"><span class="toc-text">Off-Policy Actor-Critic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Deterministic-Actor-Critic-Algorithms"><span class="toc-text">2 Deterministic Actor-Critic Algorithms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deterministic-Policy-Gradient"><span class="toc-text">Deterministic Policy Gradient</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#On-Policy-Deterministic-Actor-Critic"><span class="toc-text">On-Policy Deterministic Actor-Critic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Off-Policy-Deterministic-Actor-Critic"><span class="toc-text">Off-Policy Deterministic Actor-Critic</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DPG-Algorithm"><span class="toc-text">DPG Algorithm</span></a></li></ol>
    </div>
  </div>

  
<script src="/js/catalog.js"></script>




    
      <div class="comments-container">
        


  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>

  <div id="vcomments"></div>

  <script>
    new Valine({
      el: '#vcomments',
      appId: 'xCrix3xuQasVLFiY1a1PUTbd-gzGzoHsz',
      appKey: 'bCzXzANH4S53Q03rRywgXAGf',
      placeholder: 'Welcome!',
      avatar: 'retro'
    })
  </script>

    <style>
      .comments-container .v .vempty {
        display: none!important;
      }
    </style>




      </div>
    
  </div>


        <div class="footer">
  <div class="social">
    <ul>
      
        <li>
          <a title="github" target="_blank" rel="noopener" href="https://github.com/Yihao-Sun">
            <i class="iconfont icon-github"></i>
          </a>
        </li>
      
        <li>
          <a title="email" href="">
            <i class="iconfont icon-envelope"></i>
          </a>
        </li>
      
        <li>
          <a title="facebook" href="">
            <i class="iconfont icon-facebooksquare"></i>
          </a>
        </li>
      
        <li>
          <a title="twitter" href="">
            <i class="iconfont icon-twitter"></i>
          </a>
        </li>
      
        <li>
          <a title="微信" href="">
            <i class="iconfont icon-wechat"></i>
          </a>
        </li>
      
        <li>
          <a title="微博" href="">
            <i class="iconfont icon-weibo"></i>
          </a>
        </li>
      
        <li>
          <a title="rss" href="/atom.xml">
            <i class="iconfont icon-rss"></i>
          </a>
        </li>
      
    </ul>
  </div>
  
    <div class="footer-more">
      <a href="">Copyright © Yihao Sun 2020</a>
    </div>
  
</div>

      </div>

      <div class="back-to-top hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



      
  <div class="search-icon" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        <input type="text" class="search-input" id="search-input" placeholder="搜索...">
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile();
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)
  </script>

  
<script src="/js/search.js"></script>




    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
