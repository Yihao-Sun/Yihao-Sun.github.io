<!DOCTYPE html>
<html lang="zh-CN">

  <head>
  <meta charset="utf-8">
  <meta name="author" content="zchengsite, 1451426471@qq.com" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  
  <title>Deep Q-Network (DQN) | To be a legend</title>

  
    <link rel="apple-touch-icon" href="/images/favicon.png">
    <link rel="icon" href="/images/favicon.png">
  

  <!-- Raleway-Font -->
  <link href="https://fonts.googleapis.com/css?family=Raleway&display=swap" rel="stylesheet">

  <!-- hexo site css -->
  
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/iconfont/iconfont.css">
<link rel="stylesheet" href="/css/github-markdown.css">
<link rel="stylesheet" href="/css/highlight.css">


  <!-- jquery3.3.1 -->
  <script src="https://cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script>

  <!-- fancybox -->
  <link href="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.css" rel="stylesheet">
  <script async src="https://cdn.bootcss.com/fancybox/3.5.2/jquery.fancybox.min.js"></script>
  
<script src="/js/fancybox.js"></script>


  

<meta name="generator" content="Hexo 5.1.1"><link rel="alternate" href="/atom.xml" title="To be a legend" type="application/atom+xml">
</head>


  <body>
    <div id="app">
      <div class="header">
  <div class="avatar">
    <a href="/"><img src="/images/avatar.png" alt=""></a>
    <div class="nickname"><a href="/">To be a legend</a></div>
  </div>
  <div class="navbar">
    <ul>
      
        <li class="nav-item" data-path="/">
          <a href="/">主页</a>
        </li>
      
        <li class="nav-item" data-path="/archives/">
          <a href="/archives/">归档</a>
        </li>
      
        <li class="nav-item" data-path="/categories/">
          <a href="/categories/">分类</a>
        </li>
      
        <li class="nav-item" data-path="/tags/">
          <a href="/tags/">标签</a>
        </li>
      
        <li class="nav-item" data-path="/tools/">
          <a href="/tools/">实用工具</a>
        </li>
      
        <li class="nav-item" data-path="/about/">
          <a href="/about/">关于</a>
        </li>
      
    </ul>
  </div>
</div>


<script src="/js/activeNav.js"></script>



      <div class="flex-container">
        <!-- 文章详情页，展示文章具体内容，url形式：https://yoursite/文章标题/ -->
<!-- 同时为「标签tag」，「朋友friend」，「分类categories」，「关于about」页面的承载页面，具体展示取决于page.type -->



  

  

  

  
  <!-- 文章内容页 url形式：https://yoursite/文章标题/ -->
  <div class="container post-details" id="post-details">
    <div class="post-content">
      <div class="post-title">Deep Q-Network (DQN)</div>
      <div class="post-attach">
        <span class="post-pubtime">
          <i class="iconfont icon-updatetime" title="更新时间 | updatetime"></i>
          2021-02-01
        </span>
        
              <span class="post-tags">
                <i class="iconfont icon-tags" title="标签 | tags"></i>
                
                <span class="span--tag">
                  <a href="/tags/Reinforcement-Learning/" title="查看 Reinforcement Learning 标签">
                    <b>#</b> Reinforcement Learning
                  </a>
                </span>
                
              </span>
          
      </div>
      <div class="markdown-body">
        <p><a target="_blank" rel="noopener" href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">Paper Link</a></p>
<p>Deep Q-Network (DQN) can be regarded as the first algorithm which effectively combines reinforcement learning (RL) with deep learning (DL). Specifically, it address two main issues that hinder the combination of RL and DL: <strong>one is that in reinforcement learning an agent typically encounters sequences of highly correlated states; the other is that the learning target is constantly changed since the policy of agent is continuously updated.</strong></p>
<h2 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h2><p>Most deep learning algorithms require the data samples to be independent while in reinforment learning the data used for training are usually successive transitions which have high correlation. To alleviate the problems of correlated data, <strong>DQN utilizes an experience replay mechanism which stores a mass of transitions and these transitions will be randomly sampled for training the neural network.</strong> This idea is quite simple but effectively breaks the correlation of training data. The process is illustrated in Figure 1.</p>
<div style="width:70%;margin:auto">
  <img src="/2021/02/01/Deep-Q-Network-DQN/1.png" class="" title="Figure 1">
</div>

<p>As shown in Figure 1, transitions $(s, r, s’)$ are stored in experience replay memory when the agent interacts with the environment and meanwhile a  random minibatch of experiences $(s, a, r, s’)$ are sampled from the memory for training the network.</p>
<h2 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h2><p>The objective for training Q-Network (with weights $\theta$) is:</p>
<script type="math/tex; mode=display">
L(\theta) = E_{s, a\sim\rho(\cdot)}[(y - Q_{\theta}(s, a, \theta))^2],</script><p>where $y = r(s, a, s’) + \gamma max_{a’}Q_{\theta}(s’, a’)$. Apparently, the target $y_i$ is constantly changed with the updating of the Q-Network, which makes the update unstable. To solve the non-stationarity of the targets, they use the old version of  Q-Network (with weights $\theta’$) to compute the targets , namely $y = r(s, a, s’) + \gamma max_{a’}Q_{\theta’}(s’, a’)$ and copy $\theta$ to $\theta’$ periodically. This can make the targets stay constant for a long period of time.</p>
<h2 id="Pseudocode-of-DQN"><a href="#Pseudocode-of-DQN" class="headerlink" title="Pseudocode of DQN"></a>Pseudocode of DQN</h2><p>With the two main ideas mentioned above, the pseudocode of DQN is shown below:</p>
<hr>
<p>&emsp;Initialize value network $Q_\theta$ with random weights</p>
<p>&emsp;Copy $Q_\theta$ to create the target network $Q_{\theta’}$</p>
<p>&emsp;Initialize experience replay memory $D$ of maximal size $N$</p>
<p>&emsp;Observe the initial state $s_0$</p>
<p>&emsp;for $t\in[0, T_{total}]$:</p>
<p>&emsp;&emsp;&emsp;&emsp;With probability $\epsilon$ select a random action $a_t$ based on the behavior policy derived from $Q_\theta(s_t, a)$</p>
<p>&emsp;&emsp;&emsp;&emsp;Execute action $a_t$ in emulator and observe reward $r_t$ and next state $s_{t+1}$</p>
<p>&emsp;&emsp;&emsp;&emsp;Store $(s_t, a_t, r_t, s_{t+1})$ in $D$</p>
<p>&emsp;&emsp;&emsp;&emsp;Every $T_{train}$ steps:</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Sample a minibatch $D_s$ randomly form $D$</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;For each transition $(s, a, r, s’)$ in the minibatch:</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Predict the Q-value of the greedy action in the next state $max_{a’}Q_{\theta’}(s’, a’)$ using the target network.</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Compute the target value $y = r(s, a, s’) + \gamma max_{a’}Q_{\theta’}(s’, a’)$</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Train the value network $Q_{\theta}$ on $D_{s}$ to minimize $L(\theta)s = E_{D_s}[(y - Q_{\theta}(s, a, \theta))^2]$</p>
<p>&emsp;&emsp;&emsp;&emsp;Every $T_{target}$ steps:</p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;Update the target network with the trained value network: $\theta’\leftarrow\theta$</p>
<hr>

      </div>
      
        <div class="prev-or-next">
          <div class="post-foot-next">
            
              <a href="/2020/09/23/Git-%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%88%86%E6%94%AF/" target="_self">
                <i class="iconfont icon-chevronleft"></i>
                <span>PREV</span>
              </a>
            
          </div>
          <div class="post-attach">
            <span class="post-pubtime">
              <i class="iconfont icon-updatetime" title="更新时间 | updatetime"></i>
              2021-02-01
            </span>
            
                  <span class="post-tags">
                    <i class="iconfont icon-tags" title="标签 | tags"></i>
                    
                    <span class="span--tag">
                      <a href="/tags/Reinforcement-Learning/" title="查看 Reinforcement Learning 标签">
                        <b>#</b> Reinforcement Learning
                      </a>
                    </span>
                    
                  </span>
              
          </div>
          <div class="post-foot-prev">
            
              <a href="/2021/02/15/Deterministic-Policy-Gradient-DPG/" target="_self">
                <span>NEXT</span>
                <i class="iconfont icon-chevronright"></i>
              </a>
            
          </div>
        </div>
      
    </div>
    
  <div class="post-catalog" id="catalog">
    <div class="title">目录</div>
    <div class="catalog-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Experience-Replay"><span class="toc-text">Experience Replay</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Target-Network"><span class="toc-text">Target Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Pseudocode-of-DQN"><span class="toc-text">Pseudocode of DQN</span></a></li></ol>
    </div>
  </div>

  
<script src="/js/catalog.js"></script>




    
      <div class="comments-container">
        


  <script src='//unpkg.com/valine/dist/Valine.min.js'></script>

  <div id="vcomments"></div>

  <script>
    new Valine({
      el: '#vcomments',
      appId: 'xCrix3xuQasVLFiY1a1PUTbd-gzGzoHsz',
      appKey: 'bCzXzANH4S53Q03rRywgXAGf',
      placeholder: 'Welcome!',
      avatar: 'retro'
    })
  </script>

    <style>
      .comments-container .v .vempty {
        display: none!important;
      }
    </style>




      </div>
    
  </div>


        <div class="footer">
  <div class="social">
    <ul>
      
        <li>
          <a title="github" target="_blank" rel="noopener" href="https://github.com/Yihao-Sun">
            <i class="iconfont icon-github"></i>
          </a>
        </li>
      
        <li>
          <a title="email" href="">
            <i class="iconfont icon-envelope"></i>
          </a>
        </li>
      
        <li>
          <a title="facebook" href="">
            <i class="iconfont icon-facebooksquare"></i>
          </a>
        </li>
      
        <li>
          <a title="twitter" href="">
            <i class="iconfont icon-twitter"></i>
          </a>
        </li>
      
        <li>
          <a title="微信" href="">
            <i class="iconfont icon-wechat"></i>
          </a>
        </li>
      
        <li>
          <a title="微博" href="">
            <i class="iconfont icon-weibo"></i>
          </a>
        </li>
      
        <li>
          <a title="rss" href="/atom.xml">
            <i class="iconfont icon-rss"></i>
          </a>
        </li>
      
    </ul>
  </div>
  
    <div class="footer-more">
      <a href="">Copyright © Yihao Sun 2020</a>
    </div>
  
</div>

      </div>

      <div class="back-to-top hidden">
  <a href="javascript: void(0)">
    <i class="iconfont icon-chevronup"></i>
  </a>
</div>


<script src="/js/backtotop.js"></script>



      
  <div class="search-icon" id="search-icon">
    <a href="javascript: void(0)">
      <i class="iconfont icon-search"></i>
    </a>
  </div>

  <div class="search-overlay hidden">
    <div class="search-content" tabindex="0">
      <div class="search-title">
        <span class="search-icon-input">
          <a href="javascript: void(0)">
            <i class="iconfont icon-search"></i>
          </a>
        </span>
        <input type="text" class="search-input" id="search-input" placeholder="搜索...">
        <span class="search-close-icon" id="search-close-icon">
          <a href="javascript: void(0)">
            <i class="iconfont icon-close"></i>
          </a>
        </span>
      </div>
      <div class="search-result" id="search-result"></div>
    </div>
  </div>

  <script type="text/javascript">
    var inputArea = document.querySelector("#search-input")
    var searchOverlayArea = document.querySelector(".search-overlay")

    inputArea.onclick = function() {
      getSearchFile();
      this.onclick = null
    }

    inputArea.onkeydown = function() {
      if(event.keyCode == 13)
        return false
    }

    function openOrHideSearchContent() {
      let isHidden = searchOverlayArea.classList.contains('hidden')
      if (isHidden) {
        searchOverlayArea.classList.remove('hidden')
        document.body.classList.add('hidden')
        inputArea.focus()
      } else {
        searchOverlayArea.classList.add('hidden')
        document.body.classList.remove('hidden')
      }
    }

    function blurSearchContent(e) {
      if (e.target === searchOverlayArea) {
        openOrHideSearchContent()
      }
    }

    document.querySelector("#search-icon").addEventListener("click", openOrHideSearchContent, false)
    document.querySelector("#search-close-icon").addEventListener("click", openOrHideSearchContent, false)
    searchOverlayArea.addEventListener("click", blurSearchContent, false)
  </script>

  
<script src="/js/search.js"></script>




    </div>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":0.7}});</script></body>
</html>
